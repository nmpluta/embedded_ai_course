{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "ZL5HI3Rj3blg"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import local_utils\n",
    "from torchvision import datasets\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.transforms import ToTensor\n",
    "from local_utils import ResidualBlock\n",
    "from torch import nn"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In this part of the exercise, we will load the MiniResNet floating-point model we have learned, quantise it into a fixed-point form and finally compile it into a version suitable for Kria."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Evaluation data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by re-creating the data generator on the MNIST database:\n",
    "\n",
    "We only need the test part itself. We set `batch_size` to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset  = datasets.MNIST(root='data', train=False, download=True, transform=ToTensor()) #TODO\n",
    "test_loader  = DataLoader(dataset=test_dataset, batch_size=1, shuffle=False) #TODO"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition, we will prepare a file in `.npz` format based on the test data. We will use it to evaluate the model on the target Kria platform.\n",
    "\n",
    "Complete the `quantization_data` and `quantization_labels` vectors with data from the `test_loader`. Use the `for` loop  and `.append` to do this (an example of using the DataLoader with a `for` loop is shown in Part 1 when loading the data).\n",
    "\n",
    "Then concatenate each vector individually with the `torch.cat` function with the `dim=0` parameter and convert them to `ndarray` format using `.numpy()`. \n",
    "\n",
    "Save them with the function np.savez."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "quantization_data = []\n",
    "quantization_labels = []\n",
    "\n",
    "#TODO\n",
    "#Fill quantization vectors\n",
    "for data, label in test_loader:\n",
    "    quantization_data.append(data)\n",
    "    quantization_labels.append(label)\n",
    "\n",
    "train_X = torch.cat(quantization_data, dim=0).numpy()   #TODO\n",
    "train_Y = torch.cat(quantization_labels, dim=0).numpy() #TODO\n",
    "\n",
    "np.savez('eval_MNIST.npz', data=train_X, targets=train_Y) #TODO\n",
    "\n",
    "print('Done')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Initialisation of the floating point model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create the same MiniResNet class as in the first part of the exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MiniResNet(nn.Module):\n",
    "    def __init__(self, \n",
    "                 input_shape = (1, 28, 28), #......TODO.........\n",
    "                 num_of_classes = 10, #......TODO.........\n",
    "                 ) -> None:\n",
    "        super().__init__()\n",
    "        self.CNN = nn.Sequential(\n",
    "                                nn.Conv2d(input_shape[0], 16, 3, padding=1),\n",
    "                                nn.ReLU(),\n",
    "            \n",
    "                                ResidualBlock(16,4,3),\n",
    "    \n",
    "                                nn.Conv2d(16, 32, 3, padding=1),\n",
    "                                nn.ReLU(),\n",
    "                                nn.MaxPool2d(2,2),\n",
    "            \n",
    "                                ResidualBlock(32,4,3),\n",
    "                                ResidualBlock(32,2,3),\n",
    "\n",
    "                                nn.Conv2d(32, 64, 3, padding=1),\n",
    "                                nn.ReLU(),\n",
    "                                nn.MaxPool2d(2,2),\n",
    "                                \n",
    "\n",
    "                                ResidualBlock(64,8,3),\n",
    "                                ResidualBlock(64,16,3),\n",
    "    \n",
    "\n",
    "                                nn.Conv2d(64, 128, 3),\n",
    "                                nn.ReLU(),\n",
    "                                nn.Conv2d(128, 128, 3),\n",
    "                                nn.ReLU(),\n",
    "                                )\n",
    "        CNN_out_shape = [\n",
    "                         128,\n",
    "                         input_shape[-2]//2//2 - 3//2*2 - 3//2*2,\n",
    "                         input_shape[-1]//2//2 - 3//2*2 - 3//2*2\n",
    "                        ]\n",
    "        CNN_flatten_len = torch.prod(torch.tensor(CNN_out_shape))\n",
    "\n",
    "        self.FC = nn.Sequential(\n",
    "                                # Flatten\n",
    "                                #......TODO.........\n",
    "                                nn.Flatten(start_dim=1, end_dim=-1),\n",
    "                                # Linear (in=CNN_flatten_len, out=number of classes)\n",
    "                                #......TODO.........\n",
    "                                nn.Linear(in_features=CNN_flatten_len, out_features=num_of_classes),\n",
    "                                # Softmax (dimension = 1)\n",
    "                                #......TODO.........\n",
    "                                nn.Softmax(dim=1),\n",
    "                               )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.FC(self.CNN(x))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create the model and upload the weights from the MNIST.pth file. We write it to the device (only the CPU is available in the docker!) and set it to `.eval()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "H1qAmZ7D3jMx"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cpu') #TODO\n",
    "\n",
    "input_shape = test_loader.dataset[0][0].shape     # CHW format for MNIST\n",
    "num_of_classes = len(test_loader.dataset.classes) # Number of classes in MNIST\n",
    "net = MiniResNet(input_shape, num_of_classes) #TODO\n",
    "\n",
    "pretrainedModel = torch.load('MNIST.pth', map_location=device)  #TODO\n",
    "net.load_state_dict(pretrainedModel['model'])\n",
    "net.to(device)\n",
    "#TODO change to eval\n",
    "net.eval()\n",
    "\n",
    "print(device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Evaluation of the floating point model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before proceeding with quantisation, we will perform a quick evaluation of the floating point model. We will check that the data is properly prepared and that the model has been correctly stored and loaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "L4tpsFQX4jip"
   },
   "outputs": [],
   "source": [
    "def evaluate(model,\n",
    "             dataloader,\n",
    "             evaluator\n",
    "             ):\n",
    "    tm = local_utils.TimeMeasurement(\"Evaluation\", len(dataloader))\n",
    "    with torch.no_grad(), tm:\n",
    "        score = 0.0\n",
    "        cntr = 0\n",
    "        for i, XY in enumerate(dataloader):\n",
    "            X = XY[0]\n",
    "            Y = XY[1:]\n",
    "            y_pred = model(X)\n",
    "            score = score*cntr + X.shape[0]*evaluator(y_pred, *Y)\n",
    "            cntr += X.shape[0]\n",
    "            score /= cntr\n",
    "            print(\"\\rEvaluation {}/{}. Score = {}\".format(i,len(dataloader), score),end='')\n",
    "        \n",
    "        print(\"\\rEvaluation {}/{}. Score = {}\".format(len(dataloader),len(dataloader), score),end='\\n')\n",
    "    print(tm)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will load the Accuracy metric from `local_utils`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation 10000/10000. Score = 0.9830999970436096\n",
      "Execution time: 7.0:0.0:24:104, processed 10000 frames, throughput: 22.517220101870432 fps.\n"
     ]
    }
   ],
   "source": [
    "metric = local_utils.AccuracyMetric() #TODO\n",
    "\n",
    "# You can evaluate your floating point model first \n",
    "evaluate(net, test_loader, metric)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### If everything is working correctly and the accuracy obtained is at a similar level to the training, we can move on to quantisation."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 Quantisation of the floating point model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vitis AI Quantizer for Post Training Quantisation has two parts.\n",
    "The first is calibration (mode='calib') - Vitis AI Quantizer analyses the model and adjusts the quantisation parameters.\n",
    " \n",
    "The second part is evaluation/testing (mode='test') - the accuracy of the model is checked (there should not be much change) and the model is exported to .xmodel format.\n",
    "\n",
    "### For both parts we will use the quantize function.\n",
    "\n",
    "The function uses the quantizer for PyTorch from the Vitis AI github: https://github.com/Xilinx/Vitis-AI/tree/1.4/tools/Vitis-AI-Quantizer/vai_q_pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "QFuGuk2NB4fz",
    "outputId": "3aea7dc1-a5e2-4f08-e7e2-f2af247666e9",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def quantize(float_model:torch.nn.Module, \n",
    "             input_shape:tuple,\n",
    "             quant_dir:str, \n",
    "             quant_mode:str, \n",
    "             device:torch.device,\n",
    "             dataloader,\n",
    "             evaluator):\n",
    "    \"\"\"\n",
    "    :param float_model: float model with loaded weights\n",
    "    :param input_shape: shape of input(CH,W,H)\n",
    "    :param quant_dir: path to directory with quantized model components\n",
    "    :param quant_mode: quant_mode in ['calib', 'test'] \n",
    "    :param data_loader: data_loader - for 'calib' must be batch_size == 1\n",
    "    :param evaluator: fcn/obj like: fcn(y_pred, y_ref) -> float \n",
    "    \"\"\"\n",
    "    tm = local_utils.TimeMeasurement(\"Quantization\", len(dataloader))\n",
    "    with tm:\n",
    "        # available in docker or after packaging \n",
    "        # vitis-AI-tools/..../pytorch../pytorch_nndct\n",
    "        # and installing the package\n",
    "        from pytorch_nndct.apis import torch_quantizer, dump_xmodel\n",
    "        # model to device\n",
    "        model = float_model.to(device)\n",
    "\n",
    "        # Force to merge BN with CONV for better quantization accuracy\n",
    "        optimize = 1\n",
    "\n",
    "        rand_in = torch.randn(input_shape)\n",
    "        print(\"get qunatizer start\")\n",
    "        try:\n",
    "            quantizer = torch_quantizer(\n",
    "                quant_mode, model, rand_in, output_dir=quant_dir, device=device)\n",
    "        except Exception as e:\n",
    "            print(\"exception:\")\n",
    "            print(e)\n",
    "            return\n",
    "        print(\"get qunatizer end\")\n",
    "\n",
    "        print(\"get quantized model start\")\n",
    "        quantized_model = quantizer.quant_model\n",
    "        print(\"get quantized model end\")\n",
    "\n",
    "        # evaluate\n",
    "        print(\"testing st\")\n",
    "        evaluate(quantized_model, dataloader, evaluator)\n",
    "        print(\"testing end\")\n",
    "\n",
    "        # export config\n",
    "        if quant_mode == 'calib':\n",
    "            print(\"export config\")\n",
    "            quantizer.export_quant_config()\n",
    "            print(\"export config end\")\n",
    "        # export model\n",
    "        if quant_mode == 'test':\n",
    "            print(\"export xmodel\")\n",
    "            quantizer.export_xmodel(deploy_check=False, output_dir=quant_dir)\n",
    "            print(\"export xmodel end\")\n",
    "    print(tm)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start with calibration. We specify as input of the function:\n",
    "- float_model - the floating point model we obtained during training,\n",
    "- input_shape - the dimension of the input data in [batch, CH, W, H] format,\n",
    "- quant_dir - folder to which the quantisation result will be saved,\n",
    "- quant_mode - to choose 'calib' or 'test',\n",
    "- device - device on which the calculations will be performed (CPU),\n",
    "- dataloader - the data on which the calculations will be performed,\n",
    "- evaluator - metric according to which the accuracy will be checked\n",
    "\n",
    "### Note that quantisation in the calibration mode is slow. For large models and large data dimensions, the amount of data cannot be exaggerated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "UqCYtQlO3bll",
    "outputId": "7103e738-593a-4300-81ed-3acf6a7387e3",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No CUDA runtime is found, using CUDA_HOME='/usr/local/cuda'\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Loading NNDCT kernels...\u001b[0m\n",
      "get qunatizer start\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Quantization calibration process start up...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Quant Module is in 'cpu'.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Parsing MiniResNet...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Doing weights equalization...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Quantizable module is generated.(quant_dir/MiniResNet.py)\u001b[0m\n",
      "get qunatizer end\n",
      "get quantized model start\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Get module with quantization.\u001b[0m\n",
      "get quantized model end\n",
      "testing st\n",
      "Evaluation 10000/10000. Score = 0.9817000031471252\n",
      "Execution time: 5.0:0.0:30:416, processed 10000 frames, throughput: 30.264823388501018 fps.\n",
      "testing end\n",
      "export config\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Exporting quant config.(quant_dir/quant_info.json)\u001b[0m\n",
      "export config end\n",
      "Execution time: 5.0:0.0:31:397, processed 10000 frames, throughput: 30.175226697334523 fps.\n"
     ]
    }
   ],
   "source": [
    "# Quantize model - calib - is slow\n",
    "\n",
    "#TODO\n",
    "quantize(float_model=..., \n",
    "         input_shape=...,\n",
    "         quant_dir='quant_dir',\n",
    "         quant_mode='calib',\n",
    "         device=...,\n",
    "         dataloader=...,\n",
    "         evaluator=...\n",
    "         )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After successful calibration, it is time to test and save the model. We start the function with the mode parameter changed to 'test'.\n",
    "\n",
    "This process is faster than calibration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "hW7mQGoL3blm",
    "outputId": "ffe3ef85-8078-4f53-87cc-62a3b036c36e",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get qunatizer start\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: Quantization test process start up...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Quant Module is in 'cpu'.\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Parsing MiniResNet...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Doing weights equalization...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Quantizable module is generated.(quant_dir/MiniResNet.py)\u001b[0m\n",
      "get qunatizer end\n",
      "get quantized model start\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Get module with quantization.\u001b[0m\n",
      "get quantized model end\n",
      "testing st\n",
      "Evaluation 10000/10000. Score = 0.9824000000953674\n",
      "Execution time: 0.0:0.0:57:169, processed 10000 frames, throughput: 174.9184548731919 fps.\n",
      "testing end\n",
      "export xmodel\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Converting to xmodel ...\u001b[0m\n",
      "\n",
      "\u001b[0;32m[VAIQ_NOTE]: =>Successfully convert 'MiniResNet' to xmodel.(quant_dir/MiniResNet_int.xmodel)\u001b[0m\n",
      "export xmodel end\n",
      "Execution time: 0.0:0.0:57:426, processed 10000 frames, throughput: 174.1343355669612 fps.\n"
     ]
    }
   ],
   "source": [
    "# Quantize model - test - is faster\n",
    "\n",
    "#TODO\n",
    "quantize(float_model=..., \n",
    "         input_shape=...,\n",
    "         quant_dir='quant_dir', # directory for quantizer results\n",
    "         quant_mode='test',\n",
    "         device=...,\n",
    "         dataloader=...,\n",
    "         evaluator=...\n",
    "         )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After testing, the model should be compiled. Specify the parameters accordingly:\n",
    "\n",
    "--xmodel - path to the saved model (it is saved in the folder specified during quantisation as parameter 'quant_dir')\n",
    "--arch - we specify the arch.json file that was in the file. This is the number (fingerprint) that identifies the DPU type of the target hardware\n",
    "--net_name - the name of our model after compilation - any name\n",
    "--output_dir - the folder where the model will be saved - any name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "5MApToNQ3bln",
    "outputId": "aa19a1c1-66a9-465f-cb22-43214b612ce5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************************************************\n",
      "* VITIS_AI Compilation - Xilinx Inc.\n",
      "**************************************************\n",
      "[UNILOG][INFO] Compile mode: dpu\n",
      "[UNILOG][INFO] Debug mode: function\n",
      "[UNILOG][INFO] Target architecture: DPUCZDX8G_ISA0_B4096_MAX_BG2\n",
      "[UNILOG][INFO] Graph name: MiniResNet, with op num: 130\n",
      "[UNILOG][INFO] Begin to compile...\n",
      "[UNILOG][INFO] Total device subgraph number 3, DPU subgraph number 1\n",
      "[UNILOG][INFO] Compile done.\n",
      "[UNILOG][INFO] The meta json is saved to \"/workspace/build/meta.json\"\n",
      "[UNILOG][INFO] The compiled xmodel is saved to \"/workspace/build/MiniResNet_qu.xmodel\"\n",
      "[UNILOG][INFO] The compiled xmodel's md5sum is 6f353093d80acd35dd41b979bfe4e8e1, and has been saved to \"/workspace/build/md5sum.txt\"\n"
     ]
    }
   ],
   "source": [
    "# compile model\n",
    "!vai_c_xir --xmodel 'quant_dir/MiniResNet_int.xmodel' --arch arch.json --net_name MiniResNet_qu --output_dir build"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "ISD7IjSg3blp"
   },
   "source": [
    "We will now move on to testing the model on the target hardware.\n",
    "\n",
    "#### Version 1: Connecting to the network\n",
    "Before connecting power to the Kria, plug the USB cable into the computer and the Ethernet cable into the network where the computer is located.\n",
    "\n",
    "Connect the Kria to the power supply and wait one minute for the system to boot up.\n",
    "\n",
    "Start `cutecom` with `sudo`. Switch on the port corresponding to the Kria. If `kria login:` appears, log in:\n",
    "\n",
    "`login: ubuntu`.\n",
    "\n",
    "`password: Xilinx123`.\n",
    "\n",
    "Once logged in, system information should appear. We are interested in the `IPv4` address for `eth0`. Copy it and add `:9090` to it - example value `192.168.1.26:9090`. Paste this in your browser. Another Jupyter should appear. Log into it with the password:\n",
    "\n",
    "`xilinx`.\n",
    "\n",
    "#### Version 2: Connecting to your computer\n",
    "Before connecting the power to the Kria, connect the USB cable to the PC and the Ethernet cable between the Kria and the PC. On the PC, change the network settings `Wired Setting` -> `IPv4` -> `Shared to other computers`. Turn on power to the board.\n",
    "Start `cutecom` with `sudo`. Enable the port corresponding to Kria. If `kria login:` appears, log in:\n",
    "\n",
    "`login: ubuntu`.\n",
    "\n",
    "`password: Xilinx123`.\n",
    "\n",
    "Once logged in, system information should appear. We are interested in the `inet` address for `eth0`. Use `ifconfig` if there is no information about ethernet connection. Copy it and add `:9090` to it - example value `10.42.0.47:9090`. Paste this in your browser. Another Jupyter should appear. Log into it with the password:\n",
    "\n",
    "`xilinx`.\n",
    "\n",
    "#### Uploading files\n",
    "Create a new folder and name it `ES_Lab`. Transfer the files to it accordingly:\n",
    "- dpu.bit, \n",
    "- dpu.hwh, \n",
    "- dpu.xclbin, \n",
    "- eval_MNIST.npz or as you named your evaluation data file\n",
    "- MiniResNet_compiled.xmodel or as you named your compiled file\n",
    "\n",
    "This can be done with the `scp` command, but it is easier to drag the files from the folder into Jupyter Notebook.\n",
    "\n",
    "Translated with www.DeepL.com/Translator (free version)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ATTENTION! \n",
    "\n",
    "If there is a problem with Kria's IPv4 addresses (when several boards are connected to the same network and everyone has the same address), first check that the command `ifconfig` in the `cutecom` console works:\n",
    "\n",
    "`ifconfig`.\n",
    "\n",
    "If not, install via:\n",
    "\n",
    " `sudo apt install net-tools`.  \n",
    " \n",
    "Password = Xilinx123\n",
    "\n",
    "After that, unplug the Ethernet cable from the Kria, wait a few seconds and type in the `cutecom` console:\n",
    "\n",
    "`hostname -I`.\n",
    "\n",
    "If the console does not return any error and no IP address then type into the `cutecom` console:\n",
    "\n",
    "`sudo ifconfig eth0 192.168.1.x netmask 255.255.255.0`.\n",
    "\n",
    "Here the address given should be the same as the example value above. We set the value `x` to a different value than it was, e.g. 123. We want to avoid conflicts between boards and also computers. After that we repeat again:\n",
    "\n",
    "`hostname -I`.\n",
    "\n",
    "The address you set should appear. Plug in the Ethernet cable and start Jupyter in the browser with the set IP address. After unplugging the power supply the settings will be reset."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Quantize.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
