{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "95bd6dd5",
      "metadata": {
        "id": "95bd6dd5"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "import local_utils\n",
        "import torch.nn.functional as F\n",
        "from torchvision import datasets\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision.transforms import ToTensor\n",
        "from local_utils import ResidualBlock\n",
        "from torch import nn"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "547cd26f",
      "metadata": {
        "id": "547cd26f"
      },
      "source": [
        "### The first part of the class is dedicated to training our own network for classification. We will use the MNIST dataset and the MiniResNet model for this purpose. At the end we will do an evaluation on the CPU and GPU."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e7438742",
      "metadata": {
        "id": "e7438742"
      },
      "source": [
        "# 1. Preparation of the dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "282f626e",
      "metadata": {
        "id": "282f626e"
      },
      "source": [
        "MNIST is a database of handwritten digits. It contains 60,000 training and 10,000 test images with a size of 28 x 28. The PyTorch library enables us to retrieve the data easily using the `datasets.MNIST` function:\n",
        "- `root` - the path where the data will be saved,\n",
        "- `train` - we select whether the collection is training or testing (`True/False`),\n",
        "- `download` - if we want to download the data\n",
        "\n",
        "The downloaded and prepared data, we then pass to the DataLoaders, which will be used for training and testing:\n",
        "- `dataset` - we specify the loaded datasets,\n",
        "- `batch_size` - the amount of data in one batch. The data is not large, so you can set the parameter to 64.\n",
        "- `shuffle` - whether the data will be shuffled. This is required during training.\n",
        "\n",
        "At the end, you can check that the data has been prepared correctly. The dimension of the data batch for the MNIST set, should be (batch_size, 1, 28, 28), while the labels should be in the range 0-9.\n",
        "\n",
        "If in any doubt, please refer to the documentation of the functions used:\n",
        "\n",
        "https://pytorch.org/vision/stable/generated/torchvision.datasets.MNIST.html#torchvision.datasets.MNIST\n",
        "\n",
        "https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "f95f4de9",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f95f4de9",
        "outputId": "20d90e24-016a-47cd-90df-e3b1025bc334"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Data batch shape =  (64, 1, 28, 28)\n",
            "Labels for batch =  [1 0 6 5 1 9 0 6 8 9 9 1 6 4 1 6 0 4 1 6 2 6 7 1 3 8 6 8 0 1 2 2 1 4 0 3 7\n",
            " 5 1 6 6 0 6 3 1 0 9 0 6 8 4 2 5 7 6 9 2 3 0 7 7 8 0 8]\n"
          ]
        }
      ],
      "source": [
        "batchsize = 64\n",
        "\n",
        "#image datasets\n",
        "train_dataset = datasets.MNIST(root='data', train=True, download=True, transform=ToTensor())\n",
        "test_dataset  = datasets.MNIST(root='data', train=False, download=True, transform=ToTensor())\n",
        "\n",
        "#data loaders\n",
        "train_loader = DataLoader(dataset=train_dataset, batch_size=batchsize, shuffle=True)\n",
        "test_loader  = DataLoader(dataset=test_dataset, batch_size=batchsize, shuffle=False)\n",
        "\n",
        "for X, Y in train_loader:\n",
        "    print(\"Data batch shape = \", np.shape(X.numpy()))\n",
        "    print(\"Labels for batch = \", Y.numpy())\n",
        "    break"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f13a5972",
      "metadata": {
        "id": "f13a5972"
      },
      "source": [
        "Data batch shape =  (64, 1, 28, 28)\n",
        "Labels for batch =  [7 5 2 4 0 9 0 1 1 5 5 5 0 8 4 8 4 8 9 4 9 2 3 3 1 8 6 8 6 8 9 6 5 2 1 9 3\n",
        " 9 8 1 5 7 5 0 5 4 1 3 3 7 8 4 9 3 1 8 3 1 6 7 0 7 2 0]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3407b935",
      "metadata": {
        "id": "3407b935"
      },
      "source": [
        "# 2. Model preparation"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0f5eb3fd",
      "metadata": {
        "id": "0f5eb3fd"
      },
      "source": [
        "We create the MiniResNet class:\n",
        "- we set `input_shape`  corresponding to the data (CH, H, W). \n",
        "- we set `num_of_classes` corresponding to the data,\n",
        "\n",
        "The part responsible for feature extraction `(CNN)` is built with 2D convolution layers, Max Pooling, ReLU and residual blocks (their implementation can be found in the local_utils.py file).\n",
        "\n",
        "\n",
        "The `(FC)` classifier part, on the other hand, consists of a flattening layer, a linear layer and a softmax layer. \n",
        "\n",
        "\n",
        "PyTorch allows us to create networks using the `nn.Module` class. The `nn.sequential` container allows us to initialise the layers in the correct order. The `forward` function calls the convolution part `CNN` and then the classifier part `FC`.\n",
        "\n",
        "\n",
        "Please fill in the missing data. The principle of each layer is described in detail in the PyTorch documentation:\n",
        "\n",
        "https://pytorch.org/docs/stable/nn.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "9c44eaad",
      "metadata": {
        "id": "9c44eaad"
      },
      "outputs": [],
      "source": [
        "class MiniResNet(nn.Module):\n",
        "    def __init__(self, \n",
        "                 input_shape = (1, 28, 28), #......TODO.........\n",
        "                 num_of_classes = 10, #......TODO.........\n",
        "                 ) -> None:\n",
        "        super().__init__()\n",
        "        self.CNN = nn.Sequential(\n",
        "                                nn.Conv2d(input_shape[0], 16, 3, padding=1),\n",
        "                                nn.ReLU(),\n",
        "            \n",
        "                                ResidualBlock(16,4,3),\n",
        "    \n",
        "                                nn.Conv2d(16, 32, 3, padding=1),\n",
        "                                nn.ReLU(),\n",
        "                                nn.MaxPool2d(2,2),\n",
        "            \n",
        "                                ResidualBlock(32,4,3),\n",
        "                                ResidualBlock(32,2,3),\n",
        "\n",
        "                                nn.Conv2d(32, 64, 3, padding=1),\n",
        "                                nn.ReLU(),\n",
        "                                nn.MaxPool2d(2,2),\n",
        "                                \n",
        "\n",
        "                                ResidualBlock(64,8,3),\n",
        "                                ResidualBlock(64,16,3),\n",
        "    \n",
        "\n",
        "                                nn.Conv2d(64, 128, 3),\n",
        "                                nn.ReLU(),\n",
        "                                nn.Conv2d(128, 128, 3),\n",
        "                                nn.ReLU(),\n",
        "                                )\n",
        "        CNN_out_shape = [\n",
        "                         128,\n",
        "                         input_shape[-2]//2//2 - 3//2*2 - 3//2*2,\n",
        "                         input_shape[-1]//2//2 - 3//2*2 - 3//2*2\n",
        "                        ]\n",
        "        CNN_flatten_len = torch.prod(torch.tensor(CNN_out_shape))\n",
        "\n",
        "        self.FC = nn.Sequential(\n",
        "                                # Flatten\n",
        "                                #......TODO.........\n",
        "                                nn.Flatten(start_dim=1, end_dim=-1),\n",
        "                                # Linear (in=CNN_flatten_len, out=number of classes)\n",
        "                                #......TODO.........\n",
        "                                nn.Linear(in_features=CNN_flatten_len, out_features=num_of_classes),\n",
        "                                # Softmax (dimension = 1)\n",
        "                                #......TODO.........\n",
        "                                nn.Softmax(dim=1),\n",
        "                               )\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        return self.FC(self.CNN(x))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "85c39957",
      "metadata": {
        "id": "85c39957"
      },
      "source": [
        "We create a network instance. We can display the entire architecture using `print`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "56e63e22",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "56e63e22",
        "outputId": "42cb622f-f6c9-4f5d-e952-d5d6ac37e09a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "MiniResNet(\n",
            "  (CNN): Sequential(\n",
            "    (0): Conv2d(1, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (1): ReLU()\n",
            "    (2): ResidualBlock(\n",
            "      (L1): Sequential(\n",
            "        (0): Conv2d(16, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (1): BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (2): ReLU()\n",
            "      )\n",
            "      (L2): Sequential(\n",
            "        (0): Conv2d(4, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (2): ReLU()\n",
            "      )\n",
            "    )\n",
            "    (3): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (4): ReLU()\n",
            "    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (6): ResidualBlock(\n",
            "      (L1): Sequential(\n",
            "        (0): Conv2d(32, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (1): BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (2): ReLU()\n",
            "      )\n",
            "      (L2): Sequential(\n",
            "        (0): Conv2d(4, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (2): ReLU()\n",
            "      )\n",
            "    )\n",
            "    (7): ResidualBlock(\n",
            "      (L1): Sequential(\n",
            "        (0): Conv2d(32, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (1): BatchNorm2d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (2): ReLU()\n",
            "      )\n",
            "      (L2): Sequential(\n",
            "        (0): Conv2d(2, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (2): ReLU()\n",
            "      )\n",
            "    )\n",
            "    (8): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (9): ReLU()\n",
            "    (10): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (11): ResidualBlock(\n",
            "      (L1): Sequential(\n",
            "        (0): Conv2d(64, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (2): ReLU()\n",
            "      )\n",
            "      (L2): Sequential(\n",
            "        (0): Conv2d(8, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (2): ReLU()\n",
            "      )\n",
            "    )\n",
            "    (12): ResidualBlock(\n",
            "      (L1): Sequential(\n",
            "        (0): Conv2d(64, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (2): ReLU()\n",
            "      )\n",
            "      (L2): Sequential(\n",
            "        (0): Conv2d(16, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (2): ReLU()\n",
            "      )\n",
            "    )\n",
            "    (13): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1))\n",
            "    (14): ReLU()\n",
            "    (15): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1))\n",
            "    (16): ReLU()\n",
            "  )\n",
            "  (FC): Sequential(\n",
            "    (0): Flatten(start_dim=1, end_dim=-1)\n",
            "    (1): Linear(in_features=1152, out_features=10, bias=True)\n",
            "    (2): Softmax(dim=1)\n",
            "  )\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "input_shape = train_loader.dataset[0][0].shape     # CHW format for MNIST\n",
        "num_of_classes = len(train_loader.dataset.classes) # Number of classes in MNIST\n",
        "model = MiniResNet(input_shape, num_of_classes)\n",
        "\n",
        "print(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0458bfeb",
      "metadata": {
        "id": "0458bfeb"
      },
      "source": [
        "# 3. Network training"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0e08700e",
      "metadata": {
        "id": "0e08700e"
      },
      "source": [
        "We start by looking at the available computing resources. If possible, we want to train on the GPU (CUDA). We load the model on the device. We can check on which device we will be working.\n",
        "\n",
        "https://pytorch.org/docs/stable/tensor_attributes.html#torch.device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "4e1f62dc",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4e1f62dc",
        "outputId": "670016d2-aa48-4f37-daef-6f7b9bc36229"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cpu\n"
          ]
        }
      ],
      "source": [
        "# device = torch.device('cuda') # TODO\n",
        "device = torch.device('cpu') # TODO\n",
        "model.to(device) #TODO\n",
        "\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6e90547d",
      "metadata": {
        "id": "6e90547d"
      },
      "source": [
        "We now proceed with the training. We will use the `training` function from the local_utils.py file. It takes as arguments respectively:\n",
        "- `model` - we specify our model,\n",
        "- `train_loader` / `test_loader` - these are respectively prepared DataLoaders with training/testing data,\n",
        "- `loss_fcn` - we will use the `CrossEntropyLoss` function from the `torch.nn` library with the parameter `reduction='mean'`,\n",
        "- `metric` - we will use the Accuracy metric, implemented in the local_utils file (`AccuracyMetric`),\n",
        "- `epoch_max` - we set the number of epochs to 5 (this parameter can be modified, but a small value will make the network inaccurate, while a large value will make us wait a long time for the result and there may be an overtraining effect!),\n",
        "- `device` - specify the available device, same as with the model,\n",
        "- `update_period` - please set to `5`,\n",
        "- `optimizer` - we will use `SGD` (Stochastic Gradient Descent) from the torch.optim library. We specify the model parameters and a learning rate equal to `0.1` (please refer to the documentation).\n",
        "\n",
        "https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html\n",
        "\n",
        "https://pytorch.org/docs/stable/generated/torch.optim.SGD.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "09c8994f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "09c8994f",
        "outputId": "70f98e75-6044-4550-86a5-1e13ebeb5488"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1 / 5: STARTED\n",
            "TRAINING\n",
            "Running on platform: Linux-5.15.0-71-generic-x86_64-with-glibc2.29, machine: x86_64, python_version: 3.8.10, processor: x86_64, system: Linux, \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "938it [11:24,  1.37it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "VALIDATION\n",
            "Running on platform: Linux-5.15.0-71-generic-x86_64-with-glibc2.29, machine: x86_64, python_version: 3.8.10, processor: x86_64, system: Linux, \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "157it [00:09, 15.91it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "After epoch 1: loss=1.8641 acc=0.5960 val_loss=1.6834 val_acc=0.7779\n",
            "Epoch 1 / 5: FINISHED\n",
            "\n",
            "Epoch 2 / 5: STARTED\n",
            "TRAINING\n",
            "Running on platform: Linux-5.15.0-71-generic-x86_64-with-glibc2.29, machine: x86_64, python_version: 3.8.10, processor: x86_64, system: Linux, \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "605it [09:13,  1.59it/s]"
          ]
        }
      ],
      "source": [
        "loss_fcn = torch.nn.CrossEntropyLoss(reduction='mean') #TODO\n",
        "metric = local_utils.AccuracyMetric() #TODO\n",
        "epoch = 5 #TODO\n",
        "update_period = 5 #TODO\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.1) #TODO\n",
        "\n",
        "model, history = local_utils.training(model=model, \n",
        "                                      train_loader=train_loader, \n",
        "                                      test_loader=test_loader, \n",
        "                                      loss_fcn=loss_fcn,\n",
        "                                      metric=metric,\n",
        "                                      optimizer=optimizer,\n",
        "                                      update_period=update_period,\n",
        "                                      epoch_max=epoch,\n",
        "                                      device=device)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7b608845",
      "metadata": {
        "id": "7b608845"
      },
      "source": [
        "We can display the training history with the `plot_history` function from local_utils. We just feed the `history` value from the `training` function to it. The expected value of `acc` and `val_acc` should be at the 98% level."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "078b4b6f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 887
        },
        "id": "078b4b6f",
        "outputId": "14781b9a-b628-4b1d-839f-e89d04d6ad87"
      },
      "outputs": [],
      "source": [
        "local_utils.plot_history(history)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f61994be",
      "metadata": {
        "id": "f61994be"
      },
      "source": [
        "Save the trained model to the file `MNIST.pth`. It will be useful later in the exercise."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7c51377b",
      "metadata": {
        "id": "7c51377b"
      },
      "outputs": [],
      "source": [
        "sd = {'model': model.state_dict(), 'opt': optimizer.state_dict()}\n",
        "torch.save(sd, 'MNIST.pth', _use_new_zipfile_serialization=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f69cc13a",
      "metadata": {
        "id": "f69cc13a"
      },
      "source": [
        "# 4. Model evaluation."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "14f3efe4",
      "metadata": {
        "id": "14f3efe4"
      },
      "source": [
        "We will test the learned model on both the CPU and the GPU (if available). \n",
        "\n",
        "We start by creating a new DataLoader. We will use the test collection for it, just as we did at the beginning, but we will set the `batch_size` value to 1."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "abead943",
      "metadata": {
        "id": "abead943"
      },
      "outputs": [],
      "source": [
        "eval_loader = DataLoader(dataset=test_dataset, batch_size=1, shuffle=False) #TODO"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e5f41a72",
      "metadata": {
        "id": "e5f41a72"
      },
      "source": [
        "We set the device parameter to `cuda`.\n",
        "\n",
        "We create a new instance of the `MiniResNet` model, load the learned weights `MNIST.pth` with the `torch.load` function with the `map_location` parameter as the device on which the operation will be performed. With the `load_state_dict` function we load the data into the new model and load it onto the hardware.\n",
        "\n",
        "The `TimeMeasurement` function from `local_utils` allows us to check the time results of the model.\n",
        "\n",
        "We run the `train_test_pass` function. We specify our model, data generator, loss function, metric and device. We set `test` as `mode`. The `Optimizer` and `update_period` can be set to `None`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a8d896ec",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a8d896ec",
        "outputId": "9d38a04e-8e14-4b83-e1a4-fc846af65e55"
      },
      "outputs": [],
      "source": [
        "device = torch.device('cuda') #TODO\n",
        "\n",
        "input_shape = eval_loader.dataset[0][0].shape     # CHW format for MNIST\n",
        "num_of_classes = len(eval_loader.dataset.classes) # Number of classes in MNIST\n",
        "evalModel = MiniResNet(input_shape, num_of_classes) #TODO\n",
        "\n",
        "pretrainedModel = torch.load('MNIST.pth', map_location=device) #TODO\n",
        "evalModel.load_state_dict(pretrainedModel['model'])\n",
        "evalModel.to(device) #TODO\n",
        "\n",
        "tm = local_utils.TimeMeasurement(\"Host-GPU\", len(eval_loader))\n",
        "\n",
        "with tm:\n",
        " \n",
        "    evalModel, loss, acc = local_utils.train_test_pass(model=evalModel,\n",
        "                                                       data_generator=eval_loader,\n",
        "                                                       criterion=loss_fcn,\n",
        "                                                       metric=metric,\n",
        "                                                       optimizer=None,\n",
        "                                                       update_period=None,\n",
        "                                                       mode='test',\n",
        "                                                       device=device)\n",
        "    \n",
        "print(repr(tm))\n",
        "print(\"loss:\", loss)\n",
        "print(\"acc:\", acc)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ccfe3386",
      "metadata": {
        "id": "ccfe3386"
      },
      "source": [
        "We perform the same operation, but this time we set `cpu` as the device."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a3bfd53d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a3bfd53d",
        "outputId": "0255df37-af61-4173-8152-ee48b659ec4e"
      },
      "outputs": [],
      "source": [
        "device = torch.device('cpu') #TODO\n",
        "\n",
        "input_shape = eval_loader.dataset[0][0].shape     # CHW format for MNIST\n",
        "num_of_classes = len(eval_loader.dataset.classes) # Number of classes in MNIST\n",
        "evalModel = MiniResNet(input_shape, num_of_classes) #TODO\n",
        "\n",
        "pretrainedModel = torch.load('MNIST.pth', map_location=device) #TODO\n",
        "evalModel.load_state_dict(pretrainedModel['model'])\n",
        "evalModel.to(device) #TODO\n",
        "\n",
        "tm = local_utils.TimeMeasurement(\"Host-CPU\", len(eval_loader))\n",
        "\n",
        "with tm:\n",
        "    # TODO\n",
        "    evalModel, loss, acc = local_utils.train_test_pass(model=evalModel,\n",
        "                                                       data_generator=eval_loader,\n",
        "                                                       criterion=loss_fcn,\n",
        "                                                       metric=metric,\n",
        "                                                       optimizer=None,\n",
        "                                                       update_period=None,\n",
        "                                                       mode='test', #TODO\n",
        "                                                       device=device)\n",
        "    \n",
        "print(repr(tm))\n",
        "print(\"loss:\", loss)\n",
        "print(\"acc:\", acc)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "44076ce6",
      "metadata": {
        "id": "44076ce6"
      },
      "source": [
        "Please make a note of the performance accuracy and throughput (value in FPS). After this part, only the `MNIST.pth` file will be used. "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "982RI-XRGhiB",
      "metadata": {
        "id": "982RI-XRGhiB"
      },
      "source": [
        "**Response:** Unfortunately the GPU is not available on my computer so there is no differences in accuracy performance or either throughput."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9a296422",
      "metadata": {
        "id": "9a296422"
      },
      "source": [
        "# 5. Further exercise\n",
        "\n",
        "We will now proceed to the quantisation of the model. To do this, start the Docker image:\n",
        "\n",
        "In the same folder, start a new terminal and run the command:\n",
        "\n",
        "`sudo bash ./docker_run.sh xilinx/vitis-ai:1.4.916`.\n",
        "\n",
        "After typing your sudo password (`lsriw`), click enter until asked. Type `y` and confirm.\n",
        "When Vitis AI appears, start the conda environment for PyTorch:\n",
        "\n",
        "`conda activate vitis-ai-pytorch`.\n",
        "\n",
        "Finally, start jupyter notebook with the following command:\n",
        "\n",
        "`jupyter notebook --no-browser --ip=0.0.0.0 --NotebookApp.token='' --NotebookApp.password=''`.\n",
        "\n",
        "An http link will appear in the console. Right click and open the link. Disable this notepad and navigate to the quantize.ipynb file in running Jupyter.\n",
        "\n",
        "------------------------------------------------------------------------------------------------------\n",
        "NOTE: This is not the same Jupyter we were working now. \n",
        "The current Jupyter was running in a system conda environment. This allowed us to work with both CUDA and CPU at the same time.\n",
        "The launched Jupyter is in Docker, which is configured for quantisation with Vitis AI. There, only the CPU is available."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
